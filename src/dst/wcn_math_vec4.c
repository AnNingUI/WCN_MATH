#include "WCN/WCN_MATH_DST.h"
#include "common/wcn_math_internal.h"
#include <string.h>

// BEGIN Vec4

void WMATH_CREATE(Vec4)(DST_VEC4, const WMATH_CREATE_TYPE(Vec4) vec4_c) {
    dst->v[0] = WMATH_OR_ELSE_ZERO(vec4_c.v_x);
    dst->v[1] = WMATH_OR_ELSE_ZERO(vec4_c.v_y);
    dst->v[2] = WMATH_OR_ELSE_ZERO(vec4_c.v_z);
    dst->v[3] = WMATH_OR_ELSE_ZERO(vec4_c.v_w);
}

void WMATH_SET(Vec4)(DST_VEC4, const float x, const float y, const float z, const float w) {
    dst->v[0] = x;
    dst->v[1] = y;
    dst->v[2] = z;
    dst->v[3] = w;
}

void WMATH_COPY(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) vec4) {
    memcpy(dst, &vec4, sizeof(WMATH_TYPE(Vec4)));
}

// 0
void WMATH_ZERO(Vec4)(DST_VEC4) {
    dst->v[0] = 0.0f;
    dst->v[1] = 0.0f;
    dst->v[2] = 0.0f;
    dst->v[3] = 0.0f;
}

// 1
void WMATH_IDENTITY(Vec4)(DST_VEC4) {
    dst->v[0] = 0.0f;
    dst->v[1] = 0.0f;
    dst->v[2] = 0.0f;
    dst->v[3] = 1.0f;
}

void WMATH_CEIL(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
#ifdef __SSE4_1__
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_res = _mm_ceil_ps(vec_a);
    _mm_storeu_ps(dst->v, vec_res);
#else
    dst->v[0] = ceilf(a.v[0]);
    dst->v[1] = ceilf(a.v[1]);
    dst->v[2] = ceilf(a.v[2]);
    dst->v[3] = ceilf(a.v[3]);
#endif

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_res = vrndpq_f32(vec_a);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_res = __riscv_vfroundto_f_v_f32m1(vec_a, 0);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_res = wasm_f32x4_ceil(vec_a);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_res = __lsx_vfrintrp_s(vec_a);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = ceilf(a.v[0]);
    dst->v[1] = ceilf(a.v[1]);
    dst->v[2] = ceilf(a.v[2]);
    dst->v[3] = ceilf(a.v[3]);
#endif
}

void WMATH_FLOOR(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
#ifdef __SSE4_1__
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_res = _mm_floor_ps(vec_a);
    _mm_storeu_ps(dst->v, vec_res);
#else
    dst->v[0] = floorf(a.v[0]);
    dst->v[1] = floorf(a.v[1]);
    dst->v[2] = floorf(a.v[2]);
    dst->v[3] = floorf(a.v[3]);
#endif

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_res = vrndmq_f32(vec_a);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_res = __riscv_vfroundto_f_v_f32m1(vec_a, 1);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_res = wasm_f32x4_floor(vec_a);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_res = __lsx_vfrintrm_s(vec_a);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = floorf(a.v[0]);
    dst->v[1] = floorf(a.v[1]);
    dst->v[2] = floorf(a.v[2]);
    dst->v[3] = floorf(a.v[3]);
#endif
}

void WMATH_ROUND(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
#ifdef __SSE4_1__
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_res = _mm_round_ps(vec_a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
    _mm_storeu_ps(dst->v, vec_res);
#else
    dst->v[0] = roundf(a.v[0]);
    dst->v[1] = roundf(a.v[1]);
    dst->v[2] = roundf(a.v[2]);
    dst->v[3] = roundf(a.v[3]);
#endif

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_res = vrndnq_f32(vec_a);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_res = __riscv_vfroundto_f_v_f32m1(vec_a, 2);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_res = wasm_f32x4_nearest(vec_a);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_res = __lsx_vfrintrne_s(vec_a);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = roundf(a.v[0]);
    dst->v[1] = roundf(a.v[1]);
    dst->v[2] = roundf(a.v[2]);
    dst->v[3] = roundf(a.v[3]);
#endif
}

void WMATH_CLAMP(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const float min_val,
                       const float max_val) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_min = _mm_set1_ps(min_val);
    __m128 vec_max = _mm_set1_ps(max_val);
    __m128 vec_res = _mm_min_ps(_mm_max_ps(vec_a, vec_min), vec_max);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_min = vdupq_n_f32(min_val);
    float32x4_t vec_max = vdupq_n_f32(max_val);
    float32x4_t vec_res = vminq_f32(vmaxq_f32(vec_a, vec_min), vec_max);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_min = __riscv_vfmv_v_f_f32m1(min_val, 4);
    vfloat32m1_t vec_max = __riscv_vfmv_v_f_f32m1(max_val, 4);
    vfloat32m1_t vec_res =
        __riscv_vfmin_vv_f32m1(__riscv_vfmax_vv_f32m1(vec_a, vec_min, 4), vec_max, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_min = wasm_f32x4_splat(min_val);
    v128_t vec_max = wasm_f32x4_splat(max_val);
    v128_t vec_res = wasm_f32x4_min(wasm_f32x4_max(vec_a, vec_min), vec_max);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_min = __lsx_vldrepl_w(&min_val, 0);
    __m128 vec_max = __lsx_vldrepl_w(&max_val, 0);
    __m128 vec_res = __lsx_vfmin_s(__lsx_vfmax_s(vec_a, vec_min), vec_max);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = fminf(fmaxf(a.v[0], min_val), max_val);
    dst->v[1] = fminf(fmaxf(a.v[1], min_val), max_val);
    dst->v[2] = fminf(fmaxf(a.v[2], min_val), max_val);
    dst->v[3] = fminf(fmaxf(a.v[3], min_val), max_val);
#endif
}

void WMATH_ADD(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_add_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vaddq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfadd_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_add(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfadd_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] + b.v[0];
    dst->v[1] = a.v[1] + b.v[1];
    dst->v[2] = a.v[2] + b.v[2];
    dst->v[3] = a.v[3] + b.v[3];
#endif
}

void WMATH_ADD_SCALED(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b,
                            const float scale) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_scale = _mm_set1_ps(scale);

#if defined(WCN_HAS_FMA)
    __m128 vec_res = wcn_fma_mul_add_ps(vec_b, vec_scale, vec_a);
#else
    __m128 vec_scaled = _mm_mul_ps(vec_b, vec_scale);
    __m128 vec_res = _mm_add_ps(vec_a, vec_scaled);
#endif
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_scale = vdupq_n_f32(scale);
    float32x4_t vec_res = vmlaq_f32(vec_a, vec_b, vec_scale);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_scale = __riscv_vfmv_v_f_f32m1(scale, 4);
    vfloat32m1_t vec_scaled = __riscv_vfmul_vv_f32m1(vec_b, vec_scale, 4);
    vfloat32m1_t vec_res = __riscv_vfadd_vv_f32m1(vec_a, vec_scaled, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_scale = wasm_f32x4_splat(scale);
    v128_t vec_scaled = wasm_f32x4_mul(vec_b, vec_scale);
    v128_t vec_res = wasm_f32x4_add(vec_a, vec_scaled);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_scale = __lsx_vldrepl_w(&scale, 0);
    __m128 vec_scaled = __lsx_vfmul_s(vec_b, vec_scale);
    __m128 vec_res = __lsx_vfadd_s(vec_a, vec_scaled);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] + b.v[0] * scale;
    dst->v[1] = a.v[1] + b.v[1] * scale;
    dst->v[2] = a.v[2] + b.v[2] * scale;
    dst->v[3] = a.v[3] + b.v[3] * scale;
#endif
}

void WMATH_SUB(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_sub_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vsubq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfsub_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_sub(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfsub_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] - b.v[0];
    dst->v[1] = a.v[1] - b.v[1];
    dst->v[2] = a.v[2] - b.v[2];
    dst->v[3] = a.v[3] - b.v[3];
#endif
}

bool WMATH_EQUALS_APPROXIMATELY(Vec4)(const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
    float ep = WCN_GET_EPSILON();
    return (fabsf(a.v[0] - b.v[0]) < ep && fabsf(a.v[1] - b.v[1]) < ep &&
            fabsf(a.v[2] - b.v[2]) < ep && fabsf(a.v[3] - b.v[3]) < ep);
}

bool WMATH_EQUALS(Vec4)(const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
    return (a.v[0] == b.v[0] && a.v[1] == b.v[1] && a.v[2] == b.v[2] && a.v[3] == b.v[3]);
}

void WMATH_LERP(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b, const float t) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 va = _mm_loadu_ps(a.v);
    __m128 vb = _mm_loadu_ps(b.v);
    __m128 vt = _mm_set1_ps(t);
    __m128 vdiff = _mm_sub_ps(vb, va);
#if defined(WCN_HAS_FMA)
    __m128 vres = wcn_fma_mul_add_ps(vdiff, vt, va);
#else
    __m128 vres = _mm_add_ps(va, _mm_mul_ps(vdiff, vt));
#endif
    _mm_storeu_ps(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t va = vld1q_f32(a.v);
    float32x4_t vb = vld1q_f32(b.v);
    float32x4_t vt = vdupq_n_f32(t);
    float32x4_t vdiff = vsubq_f32(vb, va);
    float32x4_t vres = vaddq_f32(va, vmulq_f32(vdiff, vt));
    vst1q_f32(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t va = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vb = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vt = __riscv_vfmv_v_f_f32m1(t, 4);
    vfloat32m1_t vdiff = __riscv_vfsub_vv_f32m1(vb, va, 4);
    vfloat32m1_t vres = __riscv_vfmul_vv_f32m1(vdiff, vt, 4);
    vres = __riscv_vfadd_vv_f32m1(va, vres, 4);
    __riscv_vse32_v_f32m1(dst->v, vres, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t va = wasm_v128_load(a.v);
    v128_t vb = wasm_v128_load(b.v);
    v128_t vt = wasm_f32x4_splat(t);
    v128_t vdiff = wasm_f32x4_sub(vb, va);
    v128_t vres = wasm_f32x4_add(va, wasm_f32x4_mul(vdiff, vt));
    wasm_v128_store(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 va = __lsx_vld(a.v, 0);
    __m128 vb = __lsx_vld(b.v, 0);
    __m128 vt = __lsx_vldrepl_w(&t, 0);
    __m128 vdiff = __lsx_vfsub_s(vb, va);
    __m128 vres = __lsx_vfadd_s(va, __lsx_vfmul_s(vdiff, vt));
    __lsx_vst(vres, dst->v, 0);

#else
    dst->v[0] = a.v[0] + (b.v[0] - a.v[0]) * t;
    dst->v[1] = a.v[1] + (b.v[1] - a.v[1]) * t;
    dst->v[2] = a.v[2] + (b.v[2] - a.v[2]) * t;
    dst->v[3] = a.v[3] + (b.v[3] - a.v[3]) * t;
#endif
}

void WMATH_LERP_V(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b,
                        const WMATH_TYPE(Vec4) t) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 va = _mm_loadu_ps(a.v);
    __m128 vb = _mm_loadu_ps(b.v);
    __m128 vt = _mm_loadu_ps(t.v);
    __m128 vdiff = _mm_sub_ps(vb, va);
    __m128 vres = _mm_add_ps(va, _mm_mul_ps(vdiff, vt));
    _mm_storeu_ps(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t va = vld1q_f32(a.v);
    float32x4_t vb = vld1q_f32(b.v);
    float32x4_t vt = vld1q_f32(t.v);
    float32x4_t vdiff = vsubq_f32(vb, va);
    float32x4_t vres = vaddq_f32(va, vmulq_f32(vdiff, vt));
    vst1q_f32(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t va = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vb = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vt = __riscv_vle32_v_f32m1(t.v, 4);
    vfloat32m1_t vdiff = __riscv_vfsub_vv_f32m1(vb, va, 4);
    vfloat32m1_t vres = __riscv_vfmul_vv_f32m1(vdiff, vt, 4);
    vres = __riscv_vfadd_vv_f32m1(va, vres, 4);
    __riscv_vse32_v_f32m1(dst->v, vres, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t va = wasm_v128_load(a.v);
    v128_t vb = wasm_v128_load(b.v);
    v128_t vt = wasm_v128_load(t.v);
    v128_t vdiff = wasm_f32x4_sub(vb, va);
    v128_t vres = wasm_f32x4_add(va, wasm_f32x4_mul(vdiff, vt));
    wasm_v128_store(dst->v, vres);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 va = __lsx_vld(a.v, 0);
    __m128 vb = __lsx_vld(b.v, 0);
    __m128 vt = __lsx_vld(t.v, 0);
    __m128 vdiff = __lsx_vfsub_s(vb, va);
    __m128 vres = __lsx_vfadd_s(va, __lsx_vfmul_s(vdiff, vt));
    __lsx_vst(vres, dst->v, 0);

#else
    dst->v[0] = a.v[0] + (b.v[0] - a.v[0]) * t.v[0];
    dst->v[1] = a.v[1] + (b.v[1] - a.v[1]) * t.v[1];
    dst->v[2] = a.v[2] + (b.v[2] - a.v[2]) * t.v[2];
    dst->v[3] = a.v[3] + (b.v[3] - a.v[3]) * t.v[3];
#endif
}

void WMATH_FMAX(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_max_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vmaxq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfmax_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_max(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfmax_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = fmaxf(a.v[0], b.v[0]);
    dst->v[1] = fmaxf(a.v[1], b.v[1]);
    dst->v[2] = fmaxf(a.v[2], b.v[2]);
    dst->v[3] = fmaxf(a.v[3], b.v[3]);
#endif
}

void WMATH_FMIN(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_min_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vminq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfmin_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_min(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfmin_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = fminf(a.v[0], b.v[0]);
    dst->v[1] = fminf(a.v[1], b.v[1]);
    dst->v[2] = fminf(a.v[2], b.v[2]);
    dst->v[3] = fminf(a.v[3], b.v[3]);
#endif
}

void WMATH_MULTIPLY(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_mul_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vmulq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfmul_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_mul(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfmul_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] * b.v[0];
    dst->v[1] = a.v[1] * b.v[1];
    dst->v[2] = a.v[2] * b.v[2];
    dst->v[3] = a.v[3] * b.v[3];
#endif
}

void WMATH_MULTIPLY_SCALAR(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const float scalar) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_scalar = _mm_set1_ps(scalar);
    __m128 vec_res = _mm_mul_ps(vec_a, vec_scalar);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_scalar = vdupq_n_f32(scalar);
    float32x4_t vec_res = vmulq_f32(vec_a, vec_scalar);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_scalar = __riscv_vfmv_v_f_f32m1(scalar, 4);
    vfloat32m1_t vec_res = __riscv_vfmul_vv_f32m1(vec_a, vec_scalar, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_scalar = wasm_f32x4_splat(scalar);
    v128_t vec_res = wasm_f32x4_mul(vec_a, vec_scalar);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_scalar = __lsx_vldrepl_w(&scalar, 0);
    __m128 vec_res = __lsx_vfmul_s(vec_a, vec_scalar);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] * scalar;
    dst->v[1] = a.v[1] * scalar;
    dst->v[2] = a.v[2] * scalar;
    dst->v[3] = a.v[3] * scalar;
#endif
}

void WMATH_DIV(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_res = _mm_div_ps(vec_a, vec_b);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_res = vdivq_f32(vec_a, vec_b);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_res = __riscv_vfdiv_vv_f32m1(vec_a, vec_b, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_res = wasm_f32x4_div(vec_a, vec_b);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_res = __lsx_vfdiv_s(vec_a, vec_b);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] / b.v[0];
    dst->v[1] = a.v[1] / b.v[1];
    dst->v[2] = a.v[2] / b.v[2];
    dst->v[3] = a.v[3] / b.v[3];
#endif
}

void WMATH_DIV_SCALAR(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const float scalar) {
    if (scalar == 0.0f) {
        WMATH_ZERO(Vec4)(dst);
        return;
    }

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_scalar = _mm_set1_ps(scalar);
    __m128 vec_res = _mm_div_ps(vec_a, vec_scalar);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_scalar = vdupq_n_f32(scalar);
    float32x4_t vec_res = vdivq_f32(vec_a, vec_scalar);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_scalar = __riscv_vfmv_v_f_f32m1(scalar, 4);
    vfloat32m1_t vec_res = __riscv_vfdiv_vv_f32m1(vec_a, vec_scalar, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_scalar = wasm_f32x4_splat(scalar);
    v128_t vec_res = wasm_f32x4_div(vec_a, vec_scalar);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_scalar = __lsx_vldrepl_w(&scalar, 0);
    __m128 vec_res = __lsx_vfdiv_s(vec_a, vec_scalar);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = a.v[0] / scalar;
    dst->v[1] = a.v[1] / scalar;
    dst->v[2] = a.v[2] / scalar;
    dst->v[3] = a.v[3] / scalar;
#endif
}

void WMATH_INVERSE(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_one = _mm_set1_ps(1.0f);
    __m128 vec_res = _mm_div_ps(vec_one, vec_a);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_one = vdupq_n_f32(1.0f);
    float32x4_t vec_res = vdivq_f32(vec_one, vec_a);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_one = __riscv_vfmv_v_f_f32m1(1.0f, 4);
    vfloat32m1_t vec_res = __riscv_vfdiv_vv_f32m1(vec_one, vec_a, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_one = wasm_f32x4_splat(1.0f);
    v128_t vec_res = wasm_f32x4_div(vec_one, vec_a);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_one = __lsx_vldrepl_w(&1.0f, 0);
    __m128 vec_res = __lsx_vfdiv_s(vec_one, vec_a);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = 1.0f / a.v[0];
    dst->v[1] = 1.0f / a.v[1];
    dst->v[2] = 1.0f / a.v[2];
    dst->v[3] = 1.0f / a.v[3];
#endif
}

float WMATH_DOT(Vec4)(const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 vec_b = _mm_loadu_ps(b.v);
    __m128 vec_mul = _mm_mul_ps(vec_a, vec_b);
    __m128 temp = _mm_hadd_ps(vec_mul, vec_mul);
    temp = _mm_hadd_ps(temp, temp);
    return _mm_cvtss_f32(temp);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_b = vld1q_f32(b.v);
    float32x4_t vec_mul = vmulq_f32(vec_a, vec_b);
    float32x2_t low = vget_low_f32(vec_mul);
    float32x2_t high = vget_high_f32(vec_mul);
    float32x2_t sum = vadd_f32(low, high);
    sum = vpadd_f32(sum, sum);
    return vget_lane_f32(sum, 0);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t vec_b = __riscv_vle32_v_f32m1(b.v, 4);
    vfloat32m1_t vec_mul = __riscv_vfmul_vv_f32m1(vec_a, vec_b, 4);
    return __riscv_vfredusum_vs_f32m1_f32(vec_mul, __riscv_vfmv_v_f_f32m1(0.0f, 4), 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t vec_b = wasm_v128_load(b.v);
    v128_t vec_mul = wasm_f32x4_mul(vec_a, vec_b);
    v128_t v = wasm_i32x4_shuffle(vec_mul, vec_mul, 0, 1, 2, 3);
    v128_t v2 = wasm_i32x4_shuffle(vec_mul, vec_mul, 2, 3, 2, 3);
    v128_t sum = wasm_f32x4_add(v, v2);
    v = wasm_i32x4_shuffle(sum, sum, 0, 1, 0, 1);
    sum = wasm_f32x4_add(v, sum);
    return wasm_f32x4_extract_lane(sum, 0);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 vec_b = __lsx_vld(b.v, 0);
    __m128 vec_mul = __lsx_vfmul_s(vec_a, vec_b);
    __m128 high = __lsx_vshuf4i_w(vec_mul, 0x1B);
    __m128 sum = __lsx_vfadd_s(vec_mul, high);
    high = __lsx_vshuf4i_w(sum, 0x05);
    sum = __lsx_vfadd_s(sum, high);
    return __lsx_vfrep2vr_w(sum)[0];

#else
    return a.v[0] * b.v[0] + a.v[1] * b.v[1] + a.v[2] * b.v[2] + a.v[3] * b.v[3];
#endif
}

float WMATH_LENGTH_SQ(Vec4)(const WMATH_TYPE(Vec4) v) {
#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_v = _mm_load_ps(v.v);
    __m128 vec_squared = _mm_mul_ps(vec_v, vec_squared);
    __m128 hadd1 = _mm_hadd_ps(vec_squared, vec_squared);
    __m128 hadd2 = _mm_hadd_ps(hadd1, hadd1);
    return _mm_cvtss_f32(hadd2);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_v = vld1q_f32(v.v);
    float32x4_t vec_squared = vmulq_f32(vec_v, vec_v);
    float32x2_t sum = vpadd_f32(vget_low_f32(vec_squared), vget_high_f32(vec_squared));
    sum = vpadd_f32(sum, sum);
    return vget_lane_f32(sum, 0);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_v = __riscv_vle32_v_f32m1(v.v, 4);
    vfloat32m1_t vec_squared = __riscv_vfmul_vv_f32m1(vec_v, vec_v, 4);
    return __riscv_vfredusum_vs_f32m1_f32(vec_squared, __riscv_vfmv_v_f_f32m1(0.0f, 4), 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_v = wasm_v128_load(v.v);
    v128_t vec_squared = wasm_f32x4_mul(vec_v, vec_v);
    v128_t v_temp = wasm_i32x4_shuffle(vec_squared, vec_squared, 0, 1, 2, 3);
    v128_t v2 = wasm_i32x4_shuffle(vec_squared, vec_squared, 2, 3, 2, 3);
    v128_t sum = wasm_f32x4_add(v_temp, v2);
    v_temp = wasm_i32x4_shuffle(sum, sum, 0, 1, 0, 1);
    sum = wasm_f32x4_add(v_temp, sum);
    return wasm_f32x4_extract_lane(sum, 0);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_v = __lsx_vld(v.v, 0);
    __m128 vec_squared = __lsx_vfmul_s(vec_v, vec_v);
    __m128 high = __lsx_vshuf4i_w(vec_squared, 0x1B);
    __m128 sum = __lsx_vfadd_s(vec_squared, high);
    high = __lsx_vshuf4i_w(sum, 0x05);
    sum = __lsx_vfadd_s(sum, high);
    return __lsx_vfrep2vr_w(sum)[0];

#else
    return v.v[0] * v.v[0] + v.v[1] * v.v[1] + v.v[2] * v.v[2] + v.v[3] * v.v[3];
#endif
}

float WMATH_LENGTH(Vec4)(const WMATH_TYPE(Vec4) v) { return sqrtf(WMATH_LENGTH_SQ(Vec4)(v)); }

float WMATH_DISTANCE_SQ(Vec4)(const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
    float dx = a.v[0] - b.v[0];
    float dy = a.v[1] - b.v[1];
    float dz = a.v[2] - b.v[2];
    float dw = a.v[3] - b.v[3];
    return dx * dx + dy * dy + dz * dz + dw * dw;
}

float WMATH_DISTANCE(Vec4)(const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
    return sqrtf(WMATH_DISTANCE_SQ(Vec4)(a, b));
}

void WMATH_NORMALIZE(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) v) {
    const float epsilon = wcn_math_get_epsilon();

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_v = _mm_loadu_ps(v.v);
    __m128 vec_squared = _mm_mul_ps(vec_v, vec_v);
    __m128 temp = _mm_hadd_ps(vec_squared, vec_squared);
    temp = _mm_hadd_ps(temp, temp);
    float len_sq = _mm_cvtss_f32(temp);

    if (len_sq > epsilon * epsilon) {
        __m128 len_sq_vec = _mm_set_ss(len_sq);
        __m128 inv_len = wcn_fast_inv_sqrt_ps(len_sq_vec);
        __m128 inv_len_broadcast = _mm_shuffle_ps(inv_len, inv_len, _MM_SHUFFLE(0, 0, 0, 0));
        __m128 vec_res = _mm_mul_ps(vec_v, inv_len_broadcast);
        _mm_storeu_ps(dst->v, vec_res);
    } else {
        WMATH_ZERO(Vec4)(dst);
    }

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_v = vld1q_f32(v.v);
    float32x4_t vec_squared = vmulq_f32(vec_v, vec_v);
    float32x2_t low = vget_low_f32(vec_squared);
    float32x2_t high = vget_high_f32(vec_squared);
    float32x2_t sum = vadd_f32(low, high);
    sum = vpadd_f32(sum, sum);
    float len_sq = vget_lane_f32(sum, 0);

    if (len_sq > epsilon * epsilon) {
        float32x2_t len_sq_vec = vdup_n_f32(len_sq);
        float32x2_t inv_len_est = vrsqrte_f32(len_sq_vec);
        float32x2_t inv_len =
            vmul_f32(inv_len_est, vrsqrts_f32(vmul_f32(len_sq_vec, inv_len_est), inv_len_est));
        float32x4_t inv_len_broadcast = vcombine_f32(inv_len, inv_len);
        float32x4_t vec_res = vmulq_f32(vec_v, inv_len_broadcast);
        vst1q_f32(dst->v, vec_res);
    } else {
        WMATH_ZERO(Vec4)(dst);
    }

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_v = __riscv_vle32_v_f32m1(v.v, 4);
    vfloat32m1_t vec_squared = __riscv_vfmul_vv_f32m1(vec_v, vec_v, 4);
    float len_sq = __riscv_vfredusum_vs_f32m1_f32(vec_squared, __riscv_vfmv_v_f_f32m1(0.0f, 4), 4);

    if (len_sq > epsilon * epsilon) {
        float inv_len = wcn_fast_inv_sqrt(len_sq);
        vfloat32m1_t inv_len_vec = __riscv_vfmv_v_f_f32m1(inv_len, 4);
        vfloat32m1_t vec_res = __riscv_vfmul_vv_f32m1(vec_v, inv_len_vec, 4);
        __riscv_vse32_v_f32m1(dst->v, vec_res, 4);
    } else {
        WMATH_ZERO(Vec4)(dst);
    }

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_v = wasm_v128_load(v.v);
    v128_t vec_squared = wasm_f32x4_mul(vec_v, vec_v);
    v128_t v_temp = wasm_i32x4_shuffle(vec_squared, vec_squared, 0, 1, 2, 3);
    v128_t v2 = wasm_i32x4_shuffle(vec_squared, vec_squared, 2, 3, 2, 3);
    v128_t sum = wasm_f32x4_add(v_temp, v2);
    v_temp = wasm_i32x4_shuffle(sum, sum, 0, 1, 0, 1);
    sum = wasm_f32x4_add(v_temp, sum);
    float len_sq = wasm_f32x4_extract_lane(sum, 0);

    if (len_sq > epsilon * epsilon) {
        float inv_len = wcn_fast_inv_sqrt(len_sq);
        v128_t inv_len_vec = wasm_f32x4_splat(inv_len);
        v128_t vec_res = wasm_f32x4_mul(vec_v, inv_len_vec);
        wasm_v128_store(dst->v, vec_res);
    } else {
        WMATH_ZERO(Vec4)(dst);
    }

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_v = __lsx_vld(v.v, 0);
    __m128 vec_squared = __lsx_vfmul_s(vec_v, vec_v);
    __m128 high = __lsx_vshuf4i_w(vec_squared, 0x1B);
    __m128 sum = __lsx_vfadd_s(vec_squared, high);
    high = __lsx_vshuf4i_w(sum, 0x05);
    sum = __lsx_vfadd_s(sum, high);
    float len_sq = __lsx_vfrep2vr_w(sum)[0];

    if (len_sq > epsilon * epsilon) {
        float inv_len = wcn_fast_inv_sqrt(len_sq);
        __m128 inv_len_vec = __lsx_vldrepl_w(&inv_len, 0);
        __m128 vec_res = __lsx_vfmul_s(vec_v, inv_len_vec);
        __lsx_vst(vec_res, dst->v, 0);
    } else {
        WMATH_ZERO(Vec4)(dst);
    }

#else
    float len_sq = v.v[0] * v.v[0] + v.v[1] * v.v[1] + v.v[2] * v.v[2] + v.v[3] * v.v[3];

    if (len_sq > epsilon * epsilon) {
        float inv_len = wcn_fast_inv_sqrt(len_sq);
        dst->v[0] = v.v[0] * inv_len;
        dst->v[1] = v.v[1] * inv_len;
        dst->v[2] = v.v[2] * inv_len;
        dst->v[3] = v.v[3] * inv_len;
    } else {
        WMATH_ZERO(Vec4)(dst);
    }
#endif
}

void WMATH_NEGATE(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a) {

#if !defined(WMATH_DISABLE_SIMD) && WCN_HAS_X86_64
    __m128 vec_a = _mm_loadu_ps(a.v);
    __m128 sign_mask = _mm_set1_ps(-0.0f);
    __m128 vec_res = _mm_xor_ps(vec_a, sign_mask);
    _mm_storeu_ps(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_AARCH64
    float32x4_t vec_a = vld1q_f32(a.v);
    float32x4_t vec_res = vnegq_f32(vec_a);
    vst1q_f32(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_RISCV_VECTOR
    vfloat32m1_t vec_a = __riscv_vle32_v_f32m1(a.v, 4);
    vfloat32m1_t neg_one = __riscv_vfmv_v_f_f32m1(-1.0f, 4);
    vfloat32m1_t vec_res = __riscv_vfmul_vv_f32m1(vec_a, neg_one, 4);
    __riscv_vse32_v_f32m1(dst->v, vec_res, 4);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_WASM_SIMD
    v128_t vec_a = wasm_v128_load(a.v);
    v128_t neg_one = wasm_f32x4_splat(-1.0f);
    v128_t vec_res = wasm_f32x4_mul(vec_a, neg_one);
    wasm_v128_store(dst->v, vec_res);

#elif !defined(WMATH_DISABLE_SIMD) && WCN_HAS_LOONGARCH_LSX
    __m128 vec_a = __lsx_vld(a.v, 0);
    __m128 sign_mask = __lsx_vldrepl_w(&(-0.0f), 0);
    __m128 vec_res = __lsx_vxor_v(vec_a, sign_mask);
    __lsx_vst(vec_res, dst->v, 0);

#else
    dst->v[0] = -a.v[0];
    dst->v[1] = -a.v[1];
    dst->v[2] = -a.v[2];
    dst->v[3] = -a.v[3];
#endif
}

void WMATH_SET_LENGTH(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) v, const float length) {
    WMATH_NORMALIZE(Vec4)(dst, v);
    WMATH_MULTIPLY_SCALAR(Vec4)(dst, *dst, length);
}

void WMATH_TRUNCATE(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) v, const float max_length) {
    if (WMATH_LENGTH(Vec4)(v) > max_length) {
        WMATH_SET_LENGTH(Vec4)(dst, v, max_length);
    } else {
        WMATH_COPY(Vec4)(dst, v);
    }
}

void WMATH_MIDPOINT(Vec4)(DST_VEC4, const WMATH_TYPE(Vec4) a, const WMATH_TYPE(Vec4) b) {
    WMATH_LERP(Vec4)(dst, a, b, 0.5f);
}

// END Vec4
